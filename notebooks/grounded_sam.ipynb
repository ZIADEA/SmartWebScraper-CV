{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grounded Segement Anything\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m pip install -e segment_anything\n",
    "# ! python -m pip install -e GroundingDINO\n",
    "# ! pip install diffusers transformers accelerate scipy safetensors\n",
    "# pip install addict\n",
    "# pip install yapf\n",
    "# pip install pycocotools\n",
    "# pip install timm\n",
    "# pip install supervision\n",
    "# pip install segment_anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"Grounded-Segment-Anything\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement des poids du mod√®le...\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "from GroundingDINO.groundingdino.util.inference import load_model, load_image, predict\n",
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "\n",
    "# Config paths - √Ä ADAPTER √Ä VOTRE STRUCTURE\n",
    "CONFIG_PATH = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "WEIGHTS_PATH = \"GroundingDINO/weights/groundingdino_swint_ogc.pth\"\n",
    "\n",
    "# Cr√©er le dossier weights si besoin\n",
    "os.makedirs(os.path.dirname(WEIGHTS_PATH), exist_ok=True)\n",
    "\n",
    "# T√©l√©charger les poids si absent (optionnel)\n",
    "if not os.path.exists(WEIGHTS_PATH):\n",
    "    import requests\n",
    "    print(\"T√©l√©chargement des poids du mod√®le...\")\n",
    "    url = \"https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\"\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    with open(WEIGHTS_PATH, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "# Charger le mod√®le\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = load_model(CONFIG_PATH, WEIGHTS_PATH, device=device)\n",
    "\n",
    "# ... (le reste de votre code reste identique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üñºÔ∏è Traitement de AI_OR_Data_Science_OR_Machine_Learning_OR_Deep_Learning_0_1743087429.jpg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - header: 0 boxes trouv√©es\n",
      " - footer: 1 boxes trouv√©es\n",
      " - content: 0 boxes trouv√©es\n",
      " - media: 1 boxes trouv√©es\n",
      " - ads: 1 boxes trouv√©es\n",
      " - sidebar: 2 boxes trouv√©es\n",
      "‚úÖ Sauvegard√©e dans gsam_output\\AI_OR_Data_Science_OR_Machine_Learning_OR_Deep_Learning_0_1743087429.jpg\n",
      "\n",
      "üñºÔ∏è Traitement de AI_OR_Data_Science_OR_Machine_Learning_OR_Deep_Learning_0_1743313884.jpg...\n",
      " - header: 2 boxes trouv√©es\n",
      " - footer: 1 boxes trouv√©es\n",
      " - content: 1 boxes trouv√©es\n",
      " - media: 1 boxes trouv√©es\n",
      " - ads: 1 boxes trouv√©es\n",
      " - sidebar: 1 boxes trouv√©es\n",
      "‚úÖ Sauvegard√©e dans gsam_output\\AI_OR_Data_Science_OR_Machine_Learning_OR_Deep_Learning_0_1743313884.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dossier d'entr√©e\n",
    "input_dir = \"dataset_images\"\n",
    "output_dir = \"gsam_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Liste des prompts et couleurs\n",
    "prompts = {\n",
    "    \"header\": \"web page header section\",\n",
    "    \"footer\": \"bottom of web page\",\n",
    "    \"content\": \"main article or central content\",\n",
    "    \"media\": \"image or video\",\n",
    "    \"ads\": \"advertisement or banner\",\n",
    "    \"sidebar\": \"side panel or sidebar menu\"\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    \"header\": \"green\",\n",
    "    \"footer\": \"blue\",\n",
    "    \"content\": \"orange\",\n",
    "    \"media\": \"pink\",\n",
    "    \"ads\": \"red\",\n",
    "    \"sidebar\": \"yellow\"\n",
    "}\n",
    "\n",
    "# S√©lectionner 2 images\n",
    "image_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.jpg', '.png'))][:2]\n",
    "\n",
    "for img_name in image_files:\n",
    "    print(f\"\\nüñºÔ∏è Traitement de {img_name}...\")\n",
    "    img_path = os.path.join(input_dir, img_name)\n",
    "    \n",
    "    # Charger l'image\n",
    "    image_source, image = load_image(img_path)\n",
    "    \n",
    "    # Convertir pour PIL\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(image_source, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "    # Appliquer chaque prompt\n",
    "    for label, prompt in prompts.items():\n",
    "        boxes, logits, phrases = predict(\n",
    "            model=model,\n",
    "            image=image,\n",
    "            caption=prompt,\n",
    "            box_threshold=0.35,\n",
    "            text_threshold=0.25,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(f\" - {label}: {len(boxes)} boxes trouv√©es\")\n",
    "\n",
    "        # Convertir les boxes en coordonn√©es d'image\n",
    "        H, W, _ = image_source.shape\n",
    "        boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "        for box in boxes_xyxy:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=colors[label], width=3)\n",
    "            draw.text((x1 + 5, y1 + 5), label, fill=colors[label])\n",
    "\n",
    "    # Sauvegarde\n",
    "    output_path = os.path.join(output_dir, img_name)\n",
    "    pil_img.save(output_path)\n",
    "    print(f\"‚úÖ Sauvegard√©e dans {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
