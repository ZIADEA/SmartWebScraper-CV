{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des libs & enregistrement du dataset COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enregistrer ton dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances  # ‚Üê assure-toi que cette ligne est bien l√†\n",
    "\n",
    "import os\n",
    "\n",
    "# Chemin vers ton dataset\n",
    "dataset_path = \"image annotation cv_collab.v3i.coco\"\n",
    "\n",
    "# Enregistrement des datasets COCO (train + valid)\n",
    "register_coco_instances(\n",
    "    \"web_train\", {}, \n",
    "    os.path.join(dataset_path, \"train\", \"_annotations.coco.json\"),\n",
    "    os.path.join(dataset_path, \"train\")\n",
    ")\n",
    "\n",
    "register_coco_instances(\n",
    "    \"web_valid\", {}, \n",
    "    os.path.join(dataset_path, \"valid\", \"_annotations.coco.json\"),\n",
    "    os.path.join(dataset_path, \"valid\")\n",
    ")\n",
    "\n",
    "# Attribution des classes\n",
    "MetadataCatalog.get(\"web_train\").thing_classes = ['content', 'advertisement', 'footer', 'header', 'left sidebar', 'logo', 'media', 'pop up', 'right side bar']\n",
    "MetadataCatalog.get(\"web_valid\").thing_classes = ['content', 'advertisement', 'footer', 'header', 'left sidebar', 'logo', 'media', 'pop up', 'right side bar']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurer le mod√®le Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml\"))\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"web_train\",)\n",
    "cfg.DATASETS.TEST = (\"web_valid\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "\n",
    "# Utilise un mod√®le pr√©-entra√Æn√©\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml\")\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # learning rate\n",
    "cfg.SOLVER.MAX_ITER = 1000     # ajustable (300 suffisant pour 11 images)\n",
    "cfg.SOLVER.STEPS = []\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 9  # nombre de classes\n",
    "\n",
    "cfg.OUTPUT_DIR = \"./output_detectron_web\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/09 20:00:04 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): ResNet(\n",
      "    (stem): BasicStem(\n",
      "      (conv1): Conv2d(\n",
      "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (res2): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res3): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (3): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res4): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (3): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (4): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (5): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (res5): Sequential(\n",
      "      (0): BottleneckBlock(\n",
      "        (shortcut): Conv2d(\n",
      "          1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "        (conv1): Conv2d(\n",
      "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (2): BottleneckBlock(\n",
      "        (conv1): Conv2d(\n",
      "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(2048, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(2048, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=100352, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=10, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=36, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/09 20:00:04 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[04/09 20:00:04 d2.data.datasets.coco]: \u001b[0mLoaded 10 images in COCO format from image annotation cv_collab.v3i.coco\\train\\_annotations.coco.json\n",
      "\u001b[32m[04/09 20:00:04 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 10 images left.\n",
      "\u001b[32m[04/09 20:00:04 d2.data.build]: \u001b[0mDistribution of instances among all 9 categories:\n",
      "\u001b[36m|  category  | #instances   |   category    | #instances   |   category    | #instances   |\n",
      "|:----------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n",
      "|  content   | 0            | advertisement | 9            |    footer     | 9            |\n",
      "|   header   | 6            | left sidebar  | 3            |     logo      | 12           |\n",
      "|   media    | 287          |    pop up     | 6            | right side .. | 3            |\n",
      "|            |              |               |              |               |              |\n",
      "|   total    | 335          |               |              |               |              |\u001b[0m\n",
      "\u001b[32m[04/09 20:00:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[04/09 20:00:04 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[04/09 20:00:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[04/09 20:00:04 d2.data.common]: \u001b[0mSerializing 10 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/09 20:00:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.02 MiB\n",
      "\u001b[32m[04/09 20:00:04 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
      "\u001b[32m[04/09 20:00:04 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_DC5_3x/137849425/model_final_68d202.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_final_68d202.pkl: 663MB [25:54, 426kB/s]                                \n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (10, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (10,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (36, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (36,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/09 20:26:00 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\detectron2-env\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/09 20:32:14 d2.utils.events]: \u001b[0m eta: 4:27:32  iter: 19  total_loss: 4.106  loss_cls: 2.223  loss_box_reg: 0.7293  loss_rpn_cls: 0.8696  loss_rpn_loc: 0.2047    time: 17.5281  last_time: 14.9907  data_time: 0.6975  last_data_time: 0.0066   lr: 4.9953e-06  \n",
      "\u001b[32m[04/09 20:37:44 d2.utils.events]: \u001b[0m eta: 4:03:03  iter: 39  total_loss: 3.678  loss_cls: 2.068  loss_box_reg: 0.7065  loss_rpn_cls: 0.7777  loss_rpn_loc: 0.1958    time: 16.8403  last_time: 22.7729  data_time: 0.0072  last_data_time: 0.0070   lr: 9.9902e-06  \n",
      "\u001b[32m[04/09 20:43:04 d2.utils.events]: \u001b[0m eta: 3:55:04  iter: 59  total_loss: 3.24  loss_cls: 1.851  loss_box_reg: 0.6818  loss_rpn_cls: 0.4919  loss_rpn_loc: 0.1863    time: 16.5463  last_time: 21.7572  data_time: 0.0067  last_data_time: 0.0064   lr: 1.4985e-05  \n",
      "\u001b[32m[04/09 20:49:03 d2.utils.events]: \u001b[0m eta: 3:56:35  iter: 79  total_loss: 2.766  loss_cls: 1.524  loss_box_reg: 0.7403  loss_rpn_cls: 0.3509  loss_rpn_loc: 0.1654    time: 16.8977  last_time: 15.5174  data_time: 0.0049  last_data_time: 0.0050   lr: 1.998e-05  \n",
      "\u001b[32m[04/09 20:55:11 d2.utils.events]: \u001b[0m eta: 3:58:06  iter: 99  total_loss: 2.309  loss_cls: 1.165  loss_box_reg: 0.7141  loss_rpn_cls: 0.2409  loss_rpn_loc: 0.154    time: 17.2105  last_time: 24.0996  data_time: 0.0062  last_data_time: 0.0053   lr: 2.4975e-05  \n",
      "\u001b[32m[04/09 21:01:13 d2.utils.events]: \u001b[0m eta: 3:54:44  iter: 119  total_loss: 2.173  loss_cls: 0.9751  loss_box_reg: 0.7905  loss_rpn_cls: 0.1999  loss_rpn_loc: 0.1866    time: 17.3616  last_time: 20.1408  data_time: 0.0058  last_data_time: 0.0072   lr: 2.997e-05  \n",
      "\u001b[32m[04/09 21:07:27 d2.utils.events]: \u001b[0m eta: 3:50:32  iter: 139  total_loss: 1.948  loss_cls: 0.8671  loss_box_reg: 0.7517  loss_rpn_cls: 0.1581  loss_rpn_loc: 0.1591    time: 17.5543  last_time: 15.1911  data_time: 0.0065  last_data_time: 0.0048   lr: 3.4965e-05  \n",
      "\u001b[32m[04/09 21:13:29 d2.utils.events]: \u001b[0m eta: 3:46:27  iter: 159  total_loss: 1.838  loss_cls: 0.8115  loss_box_reg: 0.7085  loss_rpn_cls: 0.1326  loss_rpn_loc: 0.1495    time: 17.6219  last_time: 20.5427  data_time: 0.0058  last_data_time: 0.0056   lr: 3.996e-05  \n",
      "\u001b[32m[04/09 21:19:38 d2.utils.events]: \u001b[0m eta: 3:41:23  iter: 179  total_loss: 1.793  loss_cls: 0.7589  loss_box_reg: 0.7599  loss_rpn_cls: 0.1182  loss_rpn_loc: 0.1349    time: 17.7180  last_time: 17.2291  data_time: 0.0061  last_data_time: 0.0068   lr: 4.4955e-05  \n",
      "\u001b[32m[04/09 21:26:05 d2.utils.events]: \u001b[0m eta: 3:42:18  iter: 199  total_loss: 1.742  loss_cls: 0.7065  loss_box_reg: 0.7939  loss_rpn_cls: 0.1073  loss_rpn_loc: 0.1311    time: 17.8787  last_time: 17.0590  data_time: 0.0066  last_data_time: 0.0049   lr: 4.995e-05  \n",
      "\u001b[32m[04/09 21:32:23 d2.utils.events]: \u001b[0m eta: 3:39:37  iter: 219  total_loss: 1.688  loss_cls: 0.7093  loss_box_reg: 0.778  loss_rpn_cls: 0.09837  loss_rpn_loc: 0.132    time: 17.9739  last_time: 26.8185  data_time: 0.0071  last_data_time: 0.0069   lr: 5.4945e-05  \n",
      "\u001b[32m[04/09 21:38:51 d2.utils.events]: \u001b[0m eta: 3:34:30  iter: 239  total_loss: 1.611  loss_cls: 0.6426  loss_box_reg: 0.7138  loss_rpn_cls: 0.08922  loss_rpn_loc: 0.1344    time: 18.0945  last_time: 16.5087  data_time: 0.0080  last_data_time: 0.0077   lr: 5.994e-05  \n",
      "\u001b[32m[04/09 21:45:42 d2.utils.events]: \u001b[0m eta: 3:30:14  iter: 259  total_loss: 1.609  loss_cls: 0.6023  loss_box_reg: 0.7542  loss_rpn_cls: 0.07108  loss_rpn_loc: 0.1157    time: 18.2827  last_time: 16.4947  data_time: 0.0093  last_data_time: 0.0070   lr: 6.4935e-05  \n",
      "\u001b[32m[04/09 21:52:34 d2.utils.events]: \u001b[0m eta: 3:26:12  iter: 279  total_loss: 1.539  loss_cls: 0.5769  loss_box_reg: 0.7285  loss_rpn_cls: 0.07637  loss_rpn_loc: 0.1205    time: 18.4506  last_time: 30.2254  data_time: 0.0082  last_data_time: 0.0089   lr: 6.993e-05  \n",
      "\u001b[32m[04/09 21:59:59 d2.utils.events]: \u001b[0m eta: 3:24:06  iter: 299  total_loss: 1.525  loss_cls: 0.5607  loss_box_reg: 0.762  loss_rpn_cls: 0.06463  loss_rpn_loc: 0.1131    time: 18.7053  last_time: 19.3552  data_time: 0.0078  last_data_time: 0.0068   lr: 7.4925e-05  \n",
      "\u001b[32m[04/09 22:06:25 d2.utils.events]: \u001b[0m eta: 3:18:44  iter: 319  total_loss: 1.444  loss_cls: 0.5197  loss_box_reg: 0.7535  loss_rpn_cls: 0.07134  loss_rpn_loc: 0.1135    time: 18.7437  last_time: 12.4491  data_time: 0.0070  last_data_time: 0.0076   lr: 7.992e-05  \n",
      "\u001b[32m[04/09 22:12:57 d2.utils.events]: \u001b[0m eta: 3:13:07  iter: 339  total_loss: 1.412  loss_cls: 0.5245  loss_box_reg: 0.737  loss_rpn_cls: 0.05793  loss_rpn_loc: 0.09383    time: 18.7943  last_time: 21.2737  data_time: 0.0068  last_data_time: 0.0080   lr: 8.4915e-05  \n",
      "\u001b[32m[04/09 22:19:09 d2.utils.events]: \u001b[0m eta: 3:06:54  iter: 359  total_loss: 1.37  loss_cls: 0.4465  loss_box_reg: 0.7301  loss_rpn_cls: 0.05352  loss_rpn_loc: 0.1004    time: 18.7825  last_time: 25.7540  data_time: 0.0057  last_data_time: 0.0060   lr: 8.991e-05  \n",
      "\u001b[32m[04/09 22:25:30 d2.utils.events]: \u001b[0m eta: 3:00:58  iter: 379  total_loss: 1.325  loss_cls: 0.4543  loss_box_reg: 0.6926  loss_rpn_cls: 0.04909  loss_rpn_loc: 0.1152    time: 18.7970  last_time: 16.3999  data_time: 0.0071  last_data_time: 0.0085   lr: 9.4905e-05  \n",
      "\u001b[32m[04/09 22:31:24 d2.utils.events]: \u001b[0m eta: 2:54:06  iter: 399  total_loss: 1.198  loss_cls: 0.3946  loss_box_reg: 0.6361  loss_rpn_cls: 0.03967  loss_rpn_loc: 0.09449    time: 18.7423  last_time: 25.0069  data_time: 0.0064  last_data_time: 0.0070   lr: 9.99e-05  \n",
      "\u001b[32m[04/09 22:37:30 d2.utils.events]: \u001b[0m eta: 2:47:06  iter: 419  total_loss: 1.16  loss_cls: 0.3856  loss_box_reg: 0.6153  loss_rpn_cls: 0.04  loss_rpn_loc: 0.09557    time: 18.7195  last_time: 17.1665  data_time: 0.0058  last_data_time: 0.0084   lr: 0.0001049  \n",
      "\u001b[32m[04/09 22:43:46 d2.utils.events]: \u001b[0m eta: 2:41:14  iter: 439  total_loss: 1.091  loss_cls: 0.3765  loss_box_reg: 0.5948  loss_rpn_cls: 0.03816  loss_rpn_loc: 0.09075    time: 18.7229  last_time: 18.0497  data_time: 0.0063  last_data_time: 0.0058   lr: 0.00010989  \n",
      "\u001b[32m[04/09 22:50:16 d2.utils.events]: \u001b[0m eta: 2:36:16  iter: 459  total_loss: 1.036  loss_cls: 0.3381  loss_box_reg: 0.5698  loss_rpn_cls: 0.0362  loss_rpn_loc: 0.07953    time: 18.7566  last_time: 18.7266  data_time: 0.0060  last_data_time: 0.0039   lr: 0.00011489  \n",
      "\u001b[32m[04/09 22:56:57 d2.utils.events]: \u001b[0m eta: 2:31:44  iter: 479  total_loss: 0.8954  loss_cls: 0.2715  loss_box_reg: 0.508  loss_rpn_cls: 0.03187  loss_rpn_loc: 0.07889    time: 18.8107  last_time: 17.6870  data_time: 0.0057  last_data_time: 0.0050   lr: 0.00011988  \n",
      "\u001b[32m[04/09 23:03:34 d2.utils.events]: \u001b[0m eta: 2:25:58  iter: 499  total_loss: 0.8938  loss_cls: 0.3059  loss_box_reg: 0.4803  loss_rpn_cls: 0.02999  loss_rpn_loc: 0.0862    time: 18.8539  last_time: 27.3197  data_time: 0.0063  last_data_time: 0.0047   lr: 0.00012488  \n",
      "\u001b[32m[04/09 23:10:22 d2.utils.events]: \u001b[0m eta: 2:20:30  iter: 519  total_loss: 0.8836  loss_cls: 0.2858  loss_box_reg: 0.4891  loss_rpn_cls: 0.03007  loss_rpn_loc: 0.07575    time: 18.9136  last_time: 28.6036  data_time: 0.0072  last_data_time: 0.0057   lr: 0.00012987  \n",
      "\u001b[32m[04/09 23:17:09 d2.utils.events]: \u001b[0m eta: 2:15:04  iter: 539  total_loss: 0.8497  loss_cls: 0.2849  loss_box_reg: 0.4133  loss_rpn_cls: 0.02431  loss_rpn_loc: 0.08687    time: 18.9662  last_time: 27.7560  data_time: 0.0062  last_data_time: 0.0051   lr: 0.00013487  \n",
      "\u001b[32m[04/09 23:23:17 d2.utils.events]: \u001b[0m eta: 2:09:12  iter: 559  total_loss: 0.7838  loss_cls: 0.2471  loss_box_reg: 0.4362  loss_rpn_cls: 0.02359  loss_rpn_loc: 0.06778    time: 18.9450  last_time: 14.6805  data_time: 0.0061  last_data_time: 0.0057   lr: 0.00013986  \n",
      "\u001b[32m[04/09 23:28:30 d2.utils.events]: \u001b[0m eta: 2:03:06  iter: 579  total_loss: 0.8065  loss_cls: 0.2566  loss_box_reg: 0.4218  loss_rpn_cls: 0.024  loss_rpn_loc: 0.07652    time: 18.8311  last_time: 22.5946  data_time: 0.0054  last_data_time: 0.0054   lr: 0.00014486  \n",
      "\u001b[32m[04/09 23:33:44 d2.utils.events]: \u001b[0m eta: 1:56:56  iter: 599  total_loss: 0.805  loss_cls: 0.239  loss_box_reg: 0.4501  loss_rpn_cls: 0.02597  loss_rpn_loc: 0.07998    time: 18.7268  last_time: 13.8665  data_time: 0.0055  last_data_time: 0.0077   lr: 0.00014985  \n",
      "\u001b[32m[04/09 23:38:55 d2.utils.events]: \u001b[0m eta: 1:50:56  iter: 619  total_loss: 0.6907  loss_cls: 0.2035  loss_box_reg: 0.3919  loss_rpn_cls: 0.02019  loss_rpn_loc: 0.07489    time: 18.6243  last_time: 13.6780  data_time: 0.0056  last_data_time: 0.0079   lr: 0.00015485  \n",
      "\u001b[32m[04/09 23:44:09 d2.utils.events]: \u001b[0m eta: 1:44:57  iter: 639  total_loss: 0.6725  loss_cls: 0.2249  loss_box_reg: 0.3768  loss_rpn_cls: 0.01577  loss_rpn_loc: 0.06884    time: 18.5330  last_time: 13.9703  data_time: 0.0053  last_data_time: 0.0058   lr: 0.00015984  \n",
      "\u001b[32m[04/09 23:49:23 d2.utils.events]: \u001b[0m eta: 1:38:45  iter: 659  total_loss: 0.7076  loss_cls: 0.2155  loss_box_reg: 0.3967  loss_rpn_cls: 0.01819  loss_rpn_loc: 0.05915    time: 18.4470  last_time: 14.6399  data_time: 0.0053  last_data_time: 0.0048   lr: 0.00016484  \n",
      "\u001b[32m[04/09 23:54:36 d2.utils.events]: \u001b[0m eta: 1:32:15  iter: 679  total_loss: 0.695  loss_cls: 0.188  loss_box_reg: 0.405  loss_rpn_cls: 0.01503  loss_rpn_loc: 0.04769    time: 18.3642  last_time: 13.6775  data_time: 0.0052  last_data_time: 0.0058   lr: 0.00016983  \n",
      "\u001b[32m[04/09 23:59:49 d2.utils.events]: \u001b[0m eta: 1:26:09  iter: 699  total_loss: 0.6362  loss_cls: 0.1879  loss_box_reg: 0.3429  loss_rpn_cls: 0.01506  loss_rpn_loc: 0.05261    time: 18.2855  last_time: 13.5647  data_time: 0.0055  last_data_time: 0.0052   lr: 0.00017483  \n",
      "\u001b[32m[04/10 00:05:16 d2.utils.events]: \u001b[0m eta: 1:20:12  iter: 719  total_loss: 0.6284  loss_cls: 0.1669  loss_box_reg: 0.3685  loss_rpn_cls: 0.01322  loss_rpn_loc: 0.05496    time: 18.2323  last_time: 14.1130  data_time: 0.0055  last_data_time: 0.0045   lr: 0.00017982  \n",
      "\u001b[32m[04/10 00:10:38 d2.utils.events]: \u001b[0m eta: 1:14:24  iter: 739  total_loss: 0.6284  loss_cls: 0.1821  loss_box_reg: 0.3461  loss_rpn_cls: 0.01684  loss_rpn_loc: 0.05397    time: 18.1744  last_time: 15.6116  data_time: 0.0054  last_data_time: 0.0044   lr: 0.00018482  \n",
      "\u001b[32m[04/10 00:17:20 d2.utils.events]: \u001b[0m eta: 1:08:47  iter: 759  total_loss: 0.6448  loss_cls: 0.1848  loss_box_reg: 0.3797  loss_rpn_cls: 0.01033  loss_rpn_loc: 0.05599    time: 18.2251  last_time: 17.5621  data_time: 0.0052  last_data_time: 0.0048   lr: 0.00018981  \n",
      "\u001b[32m[04/10 00:24:28 d2.utils.events]: \u001b[0m eta: 1:03:11  iter: 779  total_loss: 0.5948  loss_cls: 0.1705  loss_box_reg: 0.3627  loss_rpn_cls: 0.01223  loss_rpn_loc: 0.05294    time: 18.3061  last_time: 16.0169  data_time: 0.0058  last_data_time: 0.0043   lr: 0.00019481  \n",
      "\u001b[32m[04/10 00:30:31 d2.utils.events]: \u001b[0m eta: 0:57:19  iter: 799  total_loss: 0.5132  loss_cls: 0.1341  loss_box_reg: 0.3251  loss_rpn_cls: 0.01175  loss_rpn_loc: 0.05407    time: 18.3027  last_time: 25.2500  data_time: 0.0062  last_data_time: 0.0058   lr: 0.0001998  \n",
      "\u001b[32m[04/10 00:36:29 d2.utils.events]: \u001b[0m eta: 0:51:30  iter: 819  total_loss: 0.5352  loss_cls: 0.1544  loss_box_reg: 0.3122  loss_rpn_cls: 0.01433  loss_rpn_loc: 0.05571    time: 18.2921  last_time: 15.8814  data_time: 0.0067  last_data_time: 0.0067   lr: 0.0002048  \n",
      "\u001b[32m[04/10 00:42:31 d2.utils.events]: \u001b[0m eta: 0:45:44  iter: 839  total_loss: 0.4978  loss_cls: 0.1368  loss_box_reg: 0.2891  loss_rpn_cls: 0.01315  loss_rpn_loc: 0.04697    time: 18.2876  last_time: 28.5531  data_time: 0.0059  last_data_time: 0.0089   lr: 0.00020979  \n",
      "\u001b[32m[04/10 00:48:36 d2.utils.events]: \u001b[0m eta: 0:39:54  iter: 859  total_loss: 0.5112  loss_cls: 0.145  loss_box_reg: 0.2993  loss_rpn_cls: 0.01431  loss_rpn_loc: 0.03779    time: 18.2874  last_time: 16.1006  data_time: 0.0061  last_data_time: 0.0054   lr: 0.00021479  \n",
      "\u001b[32m[04/10 00:54:34 d2.utils.events]: \u001b[0m eta: 0:34:05  iter: 879  total_loss: 0.4647  loss_cls: 0.1134  loss_box_reg: 0.2834  loss_rpn_cls: 0.01323  loss_rpn_loc: 0.04375    time: 18.2785  last_time: 15.9184  data_time: 0.0055  last_data_time: 0.0053   lr: 0.00021978  \n",
      "\u001b[32m[04/10 01:01:34 d2.utils.events]: \u001b[0m eta: 0:28:30  iter: 899  total_loss: 0.4891  loss_cls: 0.1328  loss_box_reg: 0.2864  loss_rpn_cls: 0.008588  loss_rpn_loc: 0.04677    time: 18.3394  last_time: 21.2070  data_time: 0.0073  last_data_time: 0.0063   lr: 0.00022478  \n",
      "\u001b[32m[04/10 01:09:10 d2.utils.events]: \u001b[0m eta: 0:22:53  iter: 919  total_loss: 0.4371  loss_cls: 0.1141  loss_box_reg: 0.2704  loss_rpn_cls: 0.01261  loss_rpn_loc: 0.04008    time: 18.4358  last_time: 21.8687  data_time: 0.0091  last_data_time: 0.0066   lr: 0.00022977  \n",
      "\u001b[32m[04/10 01:16:50 d2.utils.events]: \u001b[0m eta: 0:17:13  iter: 939  total_loss: 0.4795  loss_cls: 0.118  loss_box_reg: 0.2754  loss_rpn_cls: 0.01121  loss_rpn_loc: 0.0554    time: 18.5332  last_time: 19.7776  data_time: 0.0085  last_data_time: 0.0078   lr: 0.00023477  \n",
      "\u001b[32m[04/10 01:25:04 d2.utils.events]: \u001b[0m eta: 0:11:31  iter: 959  total_loss: 0.4579  loss_cls: 0.114  loss_box_reg: 0.2978  loss_rpn_cls: 0.00957  loss_rpn_loc: 0.03774    time: 18.6616  last_time: 36.3405  data_time: 0.0085  last_data_time: 0.0059   lr: 0.00023976  \n",
      "\u001b[32m[04/10 01:33:17 d2.utils.events]: \u001b[0m eta: 0:05:48  iter: 979  total_loss: 0.4936  loss_cls: 0.1193  loss_box_reg: 0.2972  loss_rpn_cls: 0.01331  loss_rpn_loc: 0.05238    time: 18.7841  last_time: 23.5975  data_time: 0.0123  last_data_time: 0.0102   lr: 0.00024476  \n",
      "\u001b[32m[04/10 01:41:32 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 999  total_loss: 0.4471  loss_cls: 0.1295  loss_box_reg: 0.2798  loss_rpn_cls: 0.009176  loss_rpn_loc: 0.05611    time: 18.8936  last_time: 22.5457  data_time: 0.0090  last_data_time: 0.0070   lr: 0.00024975  \n",
      "\u001b[32m[04/10 01:41:32 d2.engine.hooks]: \u001b[0mOverall training speed: 998 iterations in 5:14:15 (18.8936 s / it)\n",
      "\u001b[32m[04/10 01:41:32 d2.engine.hooks]: \u001b[0mTotal training time: 5:14:33 (0:00:17 on hooks)\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/10 01:41:32 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[04/10 01:41:32 d2.data.datasets.coco]: \u001b[0mLoaded 4 images in COCO format from image annotation cv_collab.v3i.coco\\valid\\_annotations.coco.json\n",
      "\u001b[32m[04/10 01:41:32 d2.data.build]: \u001b[0mDistribution of instances among all 9 categories:\n",
      "\u001b[36m|  category  | #instances   |   category    | #instances   |   category    | #instances   |\n",
      "|:----------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n",
      "|  content   | 0            | advertisement | 5            |    footer     | 3            |\n",
      "|   header   | 3            | left sidebar  | 0            |     logo      | 10           |\n",
      "|   media    | 97           |    pop up     | 1            | right side .. | 0            |\n",
      "|            |              |               |              |               |              |\n",
      "|   total    | 119          |               |              |               |              |\u001b[0m\n",
      "\u001b[32m[04/10 01:41:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[04/10 01:41:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[04/10 01:41:32 d2.data.common]: \u001b[0mSerializing 4 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/10 01:41:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.01 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/10 01:41:33 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n"
     ]
    }
   ],
   "source": [
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "# from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "# from detectron2.data import build_detection_test_loader\n",
    "\n",
    "# evaluator = COCOEvaluator(\"web_valid\", cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "# val_loader = build_detection_test_loader(cfg, \"web_valid\")\n",
    "\n",
    "# print(\"üß™ √âvaluation en cours...\")\n",
    "# inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = trainer.model\n",
    "# print(model)\n",
    "##couches modifi√©es\n",
    "# print(model.roi_heads.box_predictor)\n",
    "##poids appris\n",
    "# # Poids de classification\n",
    "# print(model.roi_heads.box_predictor.cls_score.weight)\n",
    "\n",
    "# # Biais\n",
    "# print(model.roi_heads.box_predictor.cls_score.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod√®le sauvegard√© ici : ./output_detectron_web\\model_final.pth\n",
      "Configuration sauvegard√©e : config.pkl\n",
      "Metadata sauvegard√©es : metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# 1. Chemin du mod√®le final entra√Æn√©\n",
    "trained_model_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "# V√©rifie que le mod√®le existe\n",
    "if os.path.exists(trained_model_path):\n",
    "    print(f\"Mod√®le sauvegard√© ici : {trained_model_path}\")\n",
    "else:\n",
    "    print(\"Aucun mod√®le trouv√©. V√©rifie que l'entra√Ænement est termin√© avec succ√®s.\")\n",
    "\n",
    "# 2. Sauvegarde de la configuration utilis√©e (fichier .pkl ou .yaml)\n",
    "with open(os.path.join(cfg.OUTPUT_DIR, \"config.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(cfg, f)\n",
    "    print(\"Configuration sauvegard√©e : config.pkl\")\n",
    "\n",
    "# 3. Sauvegarde des m√©tadonn√©es (optionnel mais conseill√©)\n",
    "metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
    "with open(os.path.join(cfg.OUTPUT_DIR, \"metadata.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(metadata, f)\n",
    "    print(\"Metadata sauvegard√©es : metadata.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/10 07:41:15 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./output_detectron_web\\model_final.pth ...\n",
      "‚úî Image annot√©e sauvegard√©e : predictions_vis_all\\annotated_articles_8_1743368487.jpg\n",
      "‚úî Image annot√©e sauvegard√©e : predictions_vis_all\\annotated_blog_OR_politics_OR_news_12_1743280851.jpg\n",
      "‚úî Image annot√©e sauvegard√©e : predictions_vis_all\\annotated_AI_OR_Data_Science_OR_Machine_Learning_OR_Deep_Learning_54_1743314761.jpg\n",
      "‚úî Image annot√©e sauvegard√©e : predictions_vis_all\\annotated_health_OR_fitness_OR_wellness_52_1743419892.jpg\n",
      "‚úî Image annot√©e sauvegard√©e : predictions_vis_all\\annotated_blog_OR_politics_OR_news_138_1743285615.jpg\n",
      "‚úî Image annot√©e sauvegard√©e : predictions_vis_all\\annotated_blog_OR_politics_OR_news_84_1743414734.jpg\n",
      "‚úî Image annot√©e sauvegard√©e : predictions_vis_all\\annotated_health_OR_fitness_OR_wellness_136_1743308641.jpg\n",
      "‚úî Image annot√©e sauvegard√©e : predictions_vis_all\\annotated_AI_OR_Data_Science_OR_Machine_Learning_OR_Deep_Learning_30_1743314354.jpg\n",
      "‚úî Image annot√©e sauvegard√©e : predictions_vis_all\\annotated_health_OR_fitness_OR_wellness_52_1743352214.jpg\n",
      "‚úî Image annot√©e sauvegard√©e : predictions_vis_all\\annotated_education__9_1743290273.jpg\n"
     ]
    }
   ],
   "source": [
    "import os, random, cv2\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "# === Chargement du mod√®le ===\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml\"))\n",
    "cfg.OUTPUT_DIR = \"./output_detectron_web\"\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 9\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# === M√©tadonn√©es ===\n",
    "thing_classes = ['content', 'advertisement', 'footer', 'header', 'left sidebar', 'logo', 'media', 'pop up', 'right side bar']\n",
    "MetadataCatalog.get(\"web_custom\").thing_classes = thing_classes\n",
    "metadata = MetadataCatalog.get(\"web_custom\")\n",
    "\n",
    "# === Couleurs RGB normalis√©es [0, 1] ===\n",
    "CLASS_COLORS = {\n",
    "    \"content\": (255, 0, 0),\n",
    "    \"advertisement\": (0, 255, 0),\n",
    "    \"footer\": (0, 0, 255),\n",
    "    \"header\": (255, 255, 0),\n",
    "    \"left sidebar\": (255, 0, 255),\n",
    "    \"logo\": (0, 255, 255),\n",
    "    \"media\": (128, 128, 0),\n",
    "    \"pop up\": (0, 128, 255),\n",
    "    \"right side bar\": (128, 0, 128),\n",
    "}\n",
    "\n",
    "# === Pr√©parer les images ===\n",
    "input_dir = \"dataset_images\"\n",
    "output_dir = \"predictions_vis_all\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "image_files = [f for f in os.listdir(input_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "selected_images = random.sample(image_files, min(10, len(image_files)))\n",
    "\n",
    "# === Annoter et enregistrer ===\n",
    "for img_name in selected_images:\n",
    "    img_path = os.path.join(input_dir, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    outputs = predictor(img)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    labels = instances.pred_classes\n",
    "    boxes = instances.pred_boxes\n",
    "    scores = instances.scores\n",
    "\n",
    "    # Ajouter le score au label\n",
    "    label_names = [f\"{thing_classes[i]}: {scores[j]:.2f}\" for j, i in enumerate(labels)]\n",
    "    colors = [tuple(c / 255 for c in CLASS_COLORS[thing_classes[i]]) for i in labels]\n",
    "\n",
    "    v = Visualizer(img[:, :, ::-1], metadata=metadata, instance_mode=ColorMode.SEGMENTATION)\n",
    "    v = v.overlay_instances(boxes=boxes, labels=label_names, assigned_colors=colors)\n",
    "    result = v.get_image()[:, :, ::-1]\n",
    "\n",
    "    # Enregistrement de l'image annot√©e\n",
    "    save_path = os.path.join(output_dir, f\"annotated_{img_name}\")\n",
    "    cv2.imwrite(save_path, result)\n",
    "    print(f\"‚úî Image annot√©e sauvegard√©e : {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/10 07:52:00 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./output_detectron_web\\model_final.pth ...\n",
      "‚úî Image sauvegard√©e : predictions_vis_header_footer\\annotated_Food_OR_Cooking_OR_Recipes_9_1743416104.jpg\n",
      "‚úî Image sauvegard√©e : predictions_vis_header_footer\\annotated_articles_23_1743369032.jpg\n",
      "‚úî Image sauvegard√©e : predictions_vis_header_footer\\annotated_AI_OR_Data_Science_OR_Machine_Learning_OR_Deep_Learning_101_1743315378.jpg\n",
      "‚úî Image sauvegard√©e : predictions_vis_header_footer\\annotated_AI_OR_Data_Science_OR_Machine_Learning_OR_Deep_Learning_144_1743316108.jpg\n",
      "‚úî Image sauvegard√©e : predictions_vis_header_footer\\annotated_education__116_1743292500.jpg\n",
      "‚úî Image sauvegard√©e : predictions_vis_header_footer\\annotated_health_OR_fitness_OR_wellness_35_1743095662.jpg\n",
      "‚úî Image sauvegard√©e : predictions_vis_header_footer\\annotated_health_OR_fitness_OR_wellness_78_1743435351.jpg\n",
      "‚úî Image sauvegard√©e : predictions_vis_header_footer\\annotated_fashion_OR_clothing_OR_style_OR_trends_86_1743367849.jpg\n",
      "‚úî Image sauvegard√©e : predictions_vis_header_footer\\annotated_AI_OR_Data_Science_OR_Machine_Learning_OR_Deep_Learning_15_1743328577.jpg\n",
      "‚úî Image sauvegard√©e : predictions_vis_header_footer\\annotated_AI_OR_Data_Science_OR_Machine_Learning_OR_Deep_Learning_189_1743317280.jpg\n"
     ]
    }
   ],
   "source": [
    "import os, random, cv2\n",
    "import torch\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.structures import Boxes\n",
    "\n",
    "\n",
    "# === Chargement du mod√®le ===\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml\"))\n",
    "cfg.OUTPUT_DIR = \"./output_detectron_web\"\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 9\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# === M√©tadonn√©es ===\n",
    "thing_classes = ['content', 'advertisement', 'footer', 'header', 'left sidebar', 'logo', 'media', 'pop up', 'right side bar']\n",
    "MetadataCatalog.get(\"web_custom\").thing_classes = thing_classes\n",
    "metadata = MetadataCatalog.get(\"web_custom\")\n",
    "\n",
    "# === Couleurs normalis√©es [0, 1] ===\n",
    "CLASS_COLORS = {\n",
    "    \"content\": (255, 0, 0),\n",
    "    \"advertisement\": (0, 255, 0),\n",
    "    \"footer\": (0, 0, 255),\n",
    "    \"header\": (255, 255, 0),\n",
    "    \"left sidebar\": (255, 0, 255),\n",
    "    \"logo\": (0, 255, 255),\n",
    "    \"media\": (128, 128, 0),\n",
    "    \"pop up\": (0, 128, 255),\n",
    "    \"right side bar\": (128, 0, 128),\n",
    "}\n",
    "\n",
    "# === Pr√©parer les images ===\n",
    "input_dir = \"dataset_images\"\n",
    "output_dir = \"predictions_vis_header_footer\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "image_files = [f for f in os.listdir(input_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "selected_images = random.sample(image_files, min(10, len(image_files)))  # modifie si tu veux toutes\n",
    "\n",
    "# === Annoter et enregistrer ===\n",
    "for img_name in selected_images:\n",
    "    img_path = os.path.join(input_dir, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    outputs = predictor(img)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    labels = instances.pred_classes\n",
    "    boxes = instances.pred_boxes\n",
    "    scores = instances.scores\n",
    "\n",
    "    selected_boxes = []\n",
    "    selected_labels = []\n",
    "    selected_colors = []\n",
    "\n",
    "    for cls_id in set(labels.tolist()):\n",
    "        cls_name = thing_classes[cls_id]\n",
    "        indices = [i for i, c in enumerate(labels) if c == cls_id]\n",
    "\n",
    "        if cls_name in [\"header\", \"footer\"]:\n",
    "            if len(indices) > 1:\n",
    "                # Choisir la box avec le score max\n",
    "                best_idx = max(indices, key=lambda i: scores[i])\n",
    "                score = scores[best_idx].item()\n",
    "                label = f\"{cls_name}: {score:.2f}\"\n",
    "                selected_boxes.append(boxes[best_idx])\n",
    "                selected_labels.append(label)\n",
    "                selected_colors.append(tuple(c / 255 for c in CLASS_COLORS[cls_name]))\n",
    "            else:\n",
    "                # Annoter normalement (seule box)\n",
    "                for i in indices:\n",
    "                    score = scores[i].item()\n",
    "                    label = f\"{cls_name}: {score:.2f}\"\n",
    "                    selected_boxes.append(boxes[i])\n",
    "                    selected_labels.append(label)\n",
    "                    selected_colors.append(tuple(c / 255 for c in CLASS_COLORS[cls_name]))\n",
    "        else:\n",
    "            # Toutes les boxes des autres classes\n",
    "            for i in indices:\n",
    "                score = scores[i].item()\n",
    "                label = f\"{cls_name}: {score:.2f}\"\n",
    "                selected_boxes.append(boxes[i])\n",
    "                selected_labels.append(label)\n",
    "                selected_colors.append(tuple(c / 255 for c in CLASS_COLORS[cls_name]))\n",
    "\n",
    "    # === Visualisation ===\n",
    "    v = Visualizer(img[:, :, ::-1], metadata=metadata, instance_mode=ColorMode.SEGMENTATION)\n",
    "    if selected_boxes:\n",
    "        if selected_boxes:\n",
    "            box_tensors = torch.stack([b.tensor[0] for b in selected_boxes])  # [N, 4]\n",
    "            selected_boxes = Boxes(box_tensors)\n",
    "        v = v.overlay_instances(\n",
    "            boxes=selected_boxes,\n",
    "            labels=selected_labels,\n",
    "            assigned_colors=selected_colors\n",
    "        )\n",
    "        result = v.get_image()[:, :, ::-1]\n",
    "    else:\n",
    "        result = img\n",
    "\n",
    "    save_path = os.path.join(output_dir, f\"annotated_{img_name}\")\n",
    "    cv2.imwrite(save_path, result)\n",
    "    print(f\"‚úî Image sauvegard√©e : {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
