Constitution et Acquisition des DonnÃ©es
========================================

La constitution d'un dataset de qualitÃ© est fondamentale pour le succÃ¨s d'un projet de Computer Vision. 
Cette section dÃ©taille notre approche pour collecter et prÃ©parer les donnÃ©es d'entraÃ®nement.

StratÃ©gie d'Acquisition
=======================

Notre stratÃ©gie s'articule autour de trois axes principaux :

1. **Collecte automatisÃ©e** via des outils de scraping web
2. **Diversification** des sources et types de contenu  
3. **Validation manuelle** pour garantir la qualitÃ©

.. mermaid::

   flowchart LR
       A[SerpAPI] --> B[Liste d'URLs]
       B --> C[Selenium + Chrome]
       C --> D[Captures d'Ã©cran]
       D --> E[~3000 images brutes]
       E --> F[Nettoyage manuel]
       F --> G[~2663 images finales]

DÃ©but de Constitution du Dataset
===============================

Capture via Selenium + undetected-chromedriver
-----------------------------------------------

La capture est effectuÃ©e avec un navigateur pilotÃ© par Selenium, combinÃ© Ã  ``undetected-chromedriver`` 
pour Ã©viter les blocages par Google ou les systÃ¨mes anti-automatisation.

.. code-block:: python

   from selenium import webdriver
   from undetected_chromedriver import Chrome
   
   # Configuration du navigateur
   options = webdriver.ChromeOptions()
   options.add_argument('--headless')
   options.add_argument('--window-size=1280,800')
   
   driver = Chrome(options=options)

Process de Collecte d'URLs
---------------------------

**Ã‰tape 1 : GÃ©nÃ©ration d'URLs via SerpAPI**

.. code-block:: python

   # RequÃªtes utilisÃ©es pour la collecte
   queries = [
       "actualitÃ©s france",
       "articles de blog",
       "sites d'information", 
       "pages produits e-commerce",
       "documentation technique",
       "forums de discussion"
   ]

.. note::
   SerpAPI gÃ©nÃ¨re des liens de pages existant sur internet Ã  partir de nos requÃªtes. 
   Pour chaque requÃªte, maximum 100 liens sont collectÃ©s et une sauvegarde des liens 
   est effectuÃ©e. Le code a Ã©tÃ© relancÃ© 5 fois pour diversifier les sources.

**Ã‰tape 2 : Configuration de la Capture**

.. list-table:: ParamÃ¨tres de Capture
   :header-rows: 1
   :widths: 30 70

   * - **ParamÃ¨tre**
     - **Valeur**
   * - Largeur fenÃªtre
     - 1280 pixels
   * - Hauteur minimum
     - 800 pixels  
   * - Hauteur maximum
     - 10000 pixels
   * - Nombre de scrolls max
     - 30
   * - Temps de pause
     - 1 seconde

**Ã‰tape 3 : Capture avec Scroll Progressif**

.. code-block:: python

   def capture_with_scroll(driver, url):
       driver.get(url)
       
       # Scroll progressif et fluide
       total_height = driver.execute_script("return document.body.scrollHeight")
       current_position = 0
       scroll_step = total_height // 30  # Max 30 scrolls
       
       while current_position < total_height:
           driver.execute_script(f"window.scrollTo(0, {current_position});")
           time.sleep(1)  # Pause pour le chargement
           current_position += scroll_step
       
       # Capture d'Ã©cran complÃ¨te
       return driver.get_screenshot_as_png()

**Ã‰tape 4 : Sauvegarde et MÃ©tadonnÃ©es**

Chaque capture est sauvegardÃ©e avec :

* L'image au format PNG
* L'URL correspondante
* La timestamp de capture
* Les dimensions de la page
* Le statut de chargement

.. code-block:: json

   {
       "filename": "capture_001.png",
       "url": "https://example.com/article",
       "timestamp": "2025-06-16T10:30:00Z",
       "dimensions": {"width": 1280, "height": 3500},
       "status": "success"
   }

RÃ©sultats de la Collecte
========================

**Volume de DonnÃ©es CollectÃ©es**

.. grid:: 3

   .. grid-item-card:: ğŸ“Š Images Brutes
      :text-align: center
      
      ~3000 captures initiales

   .. grid-item-card:: ğŸ§¹ AprÃ¨s Nettoyage
      :text-align: center
      
      2663 images conservÃ©es

   .. grid-item-card:: ğŸ¯ PriorisÃ©es
      :text-align: center
      
      Articles et vidÃ©os YouTube

Distribution des Tailles d'Images
----------------------------------

.. image:: ../_static/taille_distribution.png
   :width: 600px
   :align: center
   :alt: Distribution des tailles d'images collectÃ©es

.. warning::
   **DÃ©fis identifiÃ©s lors de la collecte :**
   
   * Certaines captures ont de trÃ¨s grandes tailles (> 5000px de hauteur)
   * Les captures trÃ¨s longues posaient des problÃ¨mes de RAM Ã  l'ouverture
   * Aucun redimensionnement appliquÃ© par choix (prÃ©servation de la rÃ©solution)

Types de Contenu PriorisÃ©s
===========================

1. **Articles de Presse et Blogs**
   
   * Structure claire avec titre, contenu, sidebar
   * PrÃ©sence frÃ©quente de publicitÃ©s
   * Bon Ã©quilibre des classes d'annotation

2. **Pages de Visualisation YouTube**
   
   * Interface standardisÃ©e
   * Ã‰lÃ©ments spÃ©cifiques : likes, vues, commentaires, recommandations
   * Excellent pour tester la prÃ©cision du modÃ¨le

3. **Sites E-commerce**
   
   * Mise en page complexe
   * Nombreux Ã©lÃ©ments visuels (images produits, prix, avis)
   * Cas d'usage rÃ©aliste pour l'application

MÃ©thode de Nettoyage
====================

Le nettoyage manuel s'est concentrÃ© sur :

.. code-block:: none

   âœ— Suppression des erreurs de domaine (pages d'erreur 404, 500)
   âœ— Ã‰limination des captures vides ou corrompues  
   âœ— Retrait des contenus non-pertinents (captchas, redirections)
   âœ“ Conservation des pages avec structure web classique
   âœ“ Priorisation du contenu textuel riche

**CritÃ¨res de Conservation :**

* PrÃ©sence de contenu textuel significatif
* Structure web reconnaissable (header, content, footer)
* QualitÃ© de capture acceptable (pas de flou majeur)
* DiversitÃ© des mises en page
* Absence d'Ã©lÃ©ments perturbateurs (pop-ups bloquants, erreurs)

TraÃ§abilitÃ© et MÃ©tadonnÃ©es
===========================

Un systÃ¨me complet de traÃ§abilitÃ© a Ã©tÃ© mis en place :

.. code-block:: python

   metadata_structure = {
       "collection_info": {
           "date_start": "2025-01-15",
           "date_end": "2025-02-28", 
           "total_queries": 30,
           "urls_collected": 3000,
           "images_captured": 2980,
           "images_kept": 2663
       },
       "quality_metrics": {
           "success_rate": 0.89,
           "avg_loading_time": 3.2,
           "error_types": ["timeout", "404", "captcha", "blocked"]
       }
   }

DÃ©fis Techniques RencontrÃ©s
============================

Gestion des Sites Modernes
---------------------------

.. list-table:: ProblÃ¨mes et Solutions
   :header-rows: 1
   :widths: 40 60

   * - **ProblÃ¨me**
     - **Solution AdoptÃ©e**
   * - Contenu chargÃ© en JavaScript
     - Attente supplÃ©mentaire aprÃ¨s scroll
   * - Protection anti-bot
     - undetected-chromedriver + rotation User-Agent
   * - Infinite scroll
     - Limitation Ã  30 scrolls maximum
   * - Pop-ups cookies/RGPD
     - Script de fermeture automatique
   * - Redirections
     - Suivi et validation de l'URL finale

Performance et Optimisation
----------------------------

**Gestion de la MÃ©moire :**

.. code-block:: python

   # Optimisations appliquÃ©es
   def optimize_memory():
       # Nettoyage cache navigateur
       driver.delete_all_cookies()
       driver.execute_script("window.localStorage.clear();")
       
       # Limitation taille images
       max_height = 10000
       if image_height > max_height:
           # Scroll partiel uniquement
           pass

**ParallÃ©lisation :**

* Utilisation de plusieurs instances Chrome
* Traitement par lots de 50 URLs
* Gestion des timeouts et reprises automatiques

Validation de la QualitÃ©
=========================

MÃ©triques de QualitÃ© Automatiques
----------------------------------

.. code-block:: python

   def validate_capture_quality(image_path):
       image = cv2.imread(image_path)
       
       # VÃ©rifications automatiques
       checks = {
           "min_height": image.shape[0] > 600,
           "min_width": image.shape[1] > 800, 
           "not_blank": cv2.countNonZero(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)) > 1000,
           "has_content": detect_text_presence(image),
           "valid_format": image is not None
       }
       
       return all(checks.values())

ContrÃ´le QualitÃ© Manuel
-----------------------

Un Ã©chantillonnage de 10% des images a Ã©tÃ© vÃ©rifiÃ© manuellement pour :

* CohÃ©rence de la structure de page
* LisibilitÃ© du contenu textuel
* PrÃ©sence des Ã©lÃ©ments web standard
* Absence d'artefacts de capture

.. tip::
   **Bonnes Pratiques IdentifiÃ©es :**
   
   * Prioriser la diversitÃ© sur la quantitÃ©
   * Maintenir une traÃ§abilitÃ© complÃ¨te
   * Valider la qualitÃ© Ã  chaque Ã©tape
   * PrÃ©server la rÃ©solution originale pour l'annotation

PrÃ©paration pour l'Annotation
==============================

Les images validÃ©es sont organisÃ©es selon la structure suivante :

.. code-block:: text

   dataset_raw/
   â”œâ”€â”€ images/
   â”‚   â”œâ”€â”€ capture_001.png
   â”‚   â”œâ”€â”€ capture_002.png
   â”‚   â””â”€â”€ ...
   â”œâ”€â”€ metadata/
   â”‚   â”œâ”€â”€ urls.json
   â”‚   â”œâ”€â”€ capture_info.json
   â”‚   â””â”€â”€ quality_report.json
   â””â”€â”€ logs/
       â”œâ”€â”€ collection.log
       â””â”€â”€ errors.log

Cette organisation facilite l'Ã©tape suivante d'annotation manuelle via Roboflow et assure une transition fluide vers la phase de modÃ©lisation.
